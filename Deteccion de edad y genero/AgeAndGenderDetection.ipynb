{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AgeAndGenderDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1Tx_7nul0qOB",
        "b2Y0ag3D0sLx",
        "a9GjDOxEYOrK",
        "EWQ-QTB1jIaz"
      ],
      "mount_file_id": "1Vuurikg9WqDnBfvuMLtwfrMAn32aggrT",
      "authorship_tag": "ABX9TyOc3YraNCUGbyC4p2f5isEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tavo826/DataScience/blob/main/AgeAndGenderDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt22N6BhR6Js"
      },
      "source": [
        "El objetivo es detectar el género y la edad de una persona a través de una imagen usando una CNN.\r\n",
        "\r\n",
        "Los géneros a predecir son 'male' y 'female' y la edad a predecir estará entre estos rangos (0 - 2), (4 - 6), (8 - 12), (15 - 20), (25 - 32), (38 - 43), (48 - 53), (60 - 100), lo que significa que la capa final será una softmax de 8 neuronas.\r\n",
        "\r\n",
        "La arquitectura de la red será:\r\n",
        " * Capa convolucional: 96 neuronas, kernel size 7\r\n",
        " * Capa convolucional: 256 neuronas, kernel size 6\r\n",
        " * Capa convolucional: 384 neuronas, kernel size 3\r\n",
        "\r\n",
        "Seguidas de 2 capas densas, cada una con 512 neuronas y una capa final softmax\r\n",
        "\r\n",
        "## Dataset\r\n",
        "\r\n",
        "El conjunto de datos datos contiene fotos de rostros e incluye varias condiciones del mundo real como ruido, iluminación, pose, y apariencia. Son un total de 26580 fotos de 2284 personnas en 8 rangos de edad\r\n",
        "\r\n",
        "Para la detección de rostros se cuenta con un archivo .pb (formato binario) que mantiene la definición de la imagen y los pesos entrenados del modelo. El archivo .pbtxt contiene el protocol buffer en formato de texto. El .prototxt descriibe la configuración o estructura de la red y el archivo .caffemodel define los estados internos de los parámetros de las capas o sea los pesos de la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw9IVx8s_Yjy"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Data Science/Detección de edad y género')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lN2DRNwCCaE"
      },
      "source": [
        "## Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tx_7nul0qOB"
      },
      "source": [
        "### .pb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "khBxHbMJf2ji",
        "outputId": "80512c4b-fe79-4e32-d5a9-5665852bc511"
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\r\n",
        "  with tf.io.gfile.GFile('saved_model.pb', 'rb') as f:\r\n",
        "    graph_def = tf.compat.v1.GraphDef()\r\n",
        "    graph_def.ParseFromString(f.read())\r\n",
        "  sess.graph.as_default()\r\n",
        "  with tf.Graph().as_default() as graph:\r\n",
        "    tf.import_graph_def(graph_def, name='')\r\n",
        "\r\n",
        "  graph_nodes = [n for n in graph_def.node]\r\n",
        "  names = []\r\n",
        "  for t in graph_nodes:\r\n",
        "    names.append(t.name)  \r\n",
        "  print(len(names))\r\n",
        "  print('Nombres de las capas de la red:')\r\n",
        "  print(names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fe2dda9ac7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdVlfOaXullK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "d56c8442-acbf-463d-8908-1037a8e9cdd9"
      },
      "source": [
        "with tf.io.gfile.GFile('saved_model.pb', 'rb') as f:\r\n",
        "  graph_def = tf.compat.v1.GraphDef()\r\n",
        "  graph_def.ParseFromString(f.read())\r\n",
        "  tf.import_graph_def(graph_def, name='')\r\n",
        "\r\n",
        "constant_values = {}\r\n",
        "\r\n",
        "with tf.compat.v1.Session() as sess:\r\n",
        "  #all_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)\r\n",
        "  constant_ops = [op for op in sess.graph.get_operations() if op.type == 'Const']\r\n",
        "  for constant_op in constant_ops:\r\n",
        "    constant_values[constant_op.name] = sess.run(constant_op.outputs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2f3d90b2550a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-xYO7C_y_lU"
      },
      "source": [
        "#Pesos y bias de la red\r\n",
        "print(len(constant_values))\r\n",
        "#print(constant_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Y0ag3D0sLx"
      },
      "source": [
        "### .pbtxt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKzX-BsA0uw4"
      },
      "source": [
        "from google.protobuf import text_format\r\n",
        "\r\n",
        "with tf.io.gfile.GFile('opencv_face_detector.pbtxt', 'r') as f:\r\n",
        "  graph_def = tf.compat.v1.GraphDef()\r\n",
        "\r\n",
        "  file_content = f.read()\r\n",
        "\r\n",
        "  text_format.Merge(file_content, graph_def)\r\n",
        "\r\n",
        "graph_def"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9GjDOxEYOrK"
      },
      "source": [
        "### .prototxt\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coEZj2leYT8B"
      },
      "source": [
        "#age_deploy\r\n",
        "age_deploy = open('age_deploy.prototxt')\r\n",
        "age = age_deploy.readlines()\r\n",
        "\r\n",
        "for line in age:\r\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRALV68JaGSO"
      },
      "source": [
        "#gender_deploy\r\n",
        "gender_deploy = open('gender_deploy.prototxt')\r\n",
        "gender = gender_deploy.readlines()\r\n",
        "\r\n",
        "for line in gender:\r\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWQ-QTB1jIaz"
      },
      "source": [
        "### .caffemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uTRL1eAnK7J"
      },
      "source": [
        "!apt install caffe-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGNLYdIliPFk"
      },
      "source": [
        "import caffe\r\n",
        "net = caffe.Net('/content/drive/MyDrive/Colab Notebooks/Data Science/Detección de edad y género/age_deploy.prototxt', \r\n",
        "                '/content/drive/MyDrive/Colab Notebooks/Data Science/Detección de edad y género/age_net.caffemodel',\r\n",
        "                caffe.TEST)\r\n",
        "pynet_ = []\r\n",
        "\r\n",
        "#Recorriendo las capas de la red\r\n",
        "for li in range(len(net.layers)):\r\n",
        "  layer = {}\r\n",
        "  layer['name'] = net._layer_names[li]\r\n",
        "  #Nombre y forma del la entrada de la capa\r\n",
        "  layer['bottoms'] = [(net._blob_names[bi], net.blobs[net._blob_names[bi]].data.shape) for bi in list(net._bottom_ids(li))]\r\n",
        "  #Nombre y forma de la salida de la capa\r\n",
        "  layer['tops'] = [(net._blob_names[bi], net.blobs[net._blob_names[bi]].data.shape) for bi in list(net._top_ids(li))]\r\n",
        "  #Tipo de capa\r\n",
        "  layer['type'] = net.layers[li].type\r\n",
        "  #Parámetros internos de la capa\r\n",
        "  layer['weights'] = [net.layers[li].blobs[bi].data[...] for bi in range(len(net.layers[li].blobs))]\r\n",
        "  pynet_.append(layer)\r\n",
        "\r\n",
        "pynet_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWSp3Nfw3UJk"
      },
      "source": [
        "### Steps\r\n",
        "\r\n",
        "Se utiliza la librería argparse para crear un generador de argumentos para así obtener el argumento de la imagen desde el símbolo del sistema.\r\n",
        "\r\n",
        "Para rostro, edad y género, se inicializa el búfer del protocolo y modelo\r\n",
        "\r\n",
        "Se inicializa los valores medios para el modelo y la lista con el rango de edades y géneros para clasificar\r\n",
        "\r\n",
        "Se usa el método readNet() para cargar las redes. El primer parámetro contiene pesos entrenados y el segundo lleva la configuración de la red.\r\n",
        "\r\n",
        "Se captura la cámara de video en caso de que se quiera clasificar lo que se muestre en la webcam\r\n",
        "\r\n",
        "Se almacena el contenido dentro hasFrame y frame\r\n",
        "\r\n",
        "Se hace una llamada a la función highlightFace() con los parámetros de faceNet y frame, lo que esto retorne será guardado en resultImg y faceBoxes\r\n",
        "\r\n",
        "El modelo net (faceNet) es la red neuronal profunda de reconocimiento facial\r\n",
        "\r\n",
        "* Se crea una copia superficial del marco que bordea el rostro y se obtiene el alto y ancho\r\n",
        "* Se crea una *gota* de la copia superficial\r\n",
        "* Configura la entrada y la ingresa a la red\r\n",
        "* faceBoxes es una lista vacía. Por cada valor entre 0 a 127, se define la confianza (0-1). Donde se encuentre una confianza mayor a la establecida (0.7), se obtienen las coordenadas y se agregan faceBoxes en forma de lista\r\n",
        "* Finalmente se ubica un rectángulo en la imagen para cada una de las listas de coordenadas y retorna la copia superficial y la lista faceBoxes\r\n",
        "\r\n",
        "Se define el rostro y se crea un objeto de 4 dimensiones desde la imagen, al hacer esto, se escala, se redimensiona y se pasan los valores medios\r\n",
        "\r\n",
        "Se alimenta la entrada para que la red obtenga la confianza de las clases. La más alta, es la seleccionada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdSnvXLpgxWe"
      },
      "source": [
        "import cv2\r\n",
        "import math\r\n",
        "import argparse\r\n",
        "\r\n",
        "def highlightFace(netm frame, conf_threshold=0.7):\r\n",
        "  frameOpencvDnn = frame.copy\r\n",
        "  frameHeight = frameOpencvDnn.shape[0]\r\n",
        "  frameWidth = frameOpencvDnn.shape[1]\r\n",
        "  blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300,300), [104,117,123], True, False)\r\n",
        "\r\n",
        "  net.setInput(blob)\r\n",
        "  detections = net.forward()\r\n",
        "  faceBoxes = []\r\n",
        "  for i in range(detections.shape[2]):\r\n",
        "    confidence = detections[0,0,i,2]\r\n",
        "    if confidence > conf_threshold:\r\n",
        "      x1 = int(detections[0,0,i,3] * frameWidth)\r\n",
        "      y1 = int(detections[0,0,i,4] * frameHeight)\r\n",
        "      x2 = int(detections[0,0,i,5] * frameWidth)\r\n",
        "      y2 = int(detections[0,0,i,6] * frameHeight)\r\n",
        "      faceBoxes.append([x1,y1,x2,y2])\r\n",
        "      cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\r\n",
        "  \r\n",
        "  return frameOpencvDnn, faceBoxes\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--image')\r\n",
        "\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "faceProto = 'open_face_detector.pbtxt'\r\n",
        "faceModel = 'saved_model.pb'\r\n",
        "ageProto = 'age_deploy.prototxt'\r\n",
        "ageModel = 'age_net.caffemodel'\r\n",
        "genderProto = 'gender_deploy.prototxt'\r\n",
        "genderModel = 'gender_net.caffemodel'\r\n",
        "\r\n",
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\r\n",
        "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\r\n",
        "genderList = ['Male', 'Female']\r\n",
        "\r\n",
        "faceNet = cv2.dnn.readNet(faceModel, faceProto)\r\n",
        "ageNet = cv2.dnn.readNet(ageModel, ageProto)\r\n",
        "genderNet = cv2.dnn.readNet(genderModel, genderProto)\r\n",
        "\r\n",
        "video = cv2.Videocapture(args.image if args.image else 0)\r\n",
        "padding = 20\r\n",
        "while cv2.waitKey(1) < 0:\r\n",
        "  hasFrame, frame = video.read()\r\n",
        "  if not hasFrame:\r\n",
        "    cv2.waitKey()\r\n",
        "    break\r\n",
        "\r\n",
        "  resultImg, faceBoxes = highlightFace(faceNet, frame)\r\n",
        "  if nos faceBoxes:\r\n",
        "    print('No face detected')\r\n",
        "\r\n",
        "  for faceBox in faceBoxes:\r\n",
        "    face = frame[max(0, faceBox[1] - padding):\r\n",
        "                min(faceBox[3] + padding, frame.shape[0] - 1, max(0,faceBox[0] - padding))\r\n",
        "                :min(faceBox[2] + padding, frame.shape[1] - 1)]\r\n",
        "            \r\n",
        "    blob = cv2.dnn.blobFromImage(face, 1.0, (277,277), MODEL_MEAN_VALUES, swapRB=False)\r\n",
        "    genderNet.setInput(blob)\r\n",
        "    genderPreds = genderNet.forward()\r\n",
        "    gender = genderList[genderPreds[0].argmax()]\r\n",
        "    print(f'Gender: {gender}')\r\n",
        "\r\n",
        "    ageNet.setInput(blob)\r\n",
        "    agePreds = ageNet.forward()\r\n",
        "    age = ageList[agePreds[0].argmax()]\r\n",
        "    print(f'Age: {age[1:-1]} years')\r\n",
        "\r\n",
        "    cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBo[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\r\n",
        "    cv2-imshow(\"Detecting age and gender\", resultImg)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
